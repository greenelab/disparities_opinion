---
title: Computational audits combat disparities in recognition
keywords:
- markdown
- publishing
- manubot
lang: en-US
date-meta: '2021-11-10'
author-meta:
- John Doe
- Jane Roe
header-includes: |-
  <!--
  Manubot generated metadata rendered from header-includes-template.html.
  Suggest improvements at https://github.com/manubot/manubot/blob/main/manubot/process/header-includes-template.html
  -->
  <meta name="dc.format" content="text/html" />
  <meta name="dc.title" content="Computational audits combat disparities in recognition" />
  <meta name="citation_title" content="Computational audits combat disparities in recognition" />
  <meta property="og:title" content="Computational audits combat disparities in recognition" />
  <meta property="twitter:title" content="Computational audits combat disparities in recognition" />
  <meta name="dc.date" content="2021-11-10" />
  <meta name="citation_publication_date" content="2021-11-10" />
  <meta name="dc.language" content="en-US" />
  <meta name="citation_language" content="en-US" />
  <meta name="dc.relation.ispartof" content="Manubot" />
  <meta name="dc.publisher" content="Manubot" />
  <meta name="citation_journal_title" content="Manubot" />
  <meta name="citation_technical_report_institution" content="Manubot" />
  <meta name="citation_author" content="John Doe" />
  <meta name="citation_author_institution" content="Department of Something, University of Whatever" />
  <meta name="citation_author_orcid" content="XXXX-XXXX-XXXX-XXXX" />
  <meta name="twitter:creator" content="@johndoe" />
  <meta name="citation_author" content="Jane Roe" />
  <meta name="citation_author_institution" content="Department of Something, University of Whatever" />
  <meta name="citation_author_institution" content="Department of Whatever, University of Something" />
  <meta name="citation_author_orcid" content="XXXX-XXXX-XXXX-XXXX" />
  <link rel="canonical" href="https://greenelab.github.io/disparities_opinion/" />
  <meta property="og:url" content="https://greenelab.github.io/disparities_opinion/" />
  <meta property="twitter:url" content="https://greenelab.github.io/disparities_opinion/" />
  <meta name="citation_fulltext_html_url" content="https://greenelab.github.io/disparities_opinion/" />
  <meta name="citation_pdf_url" content="https://greenelab.github.io/disparities_opinion/manuscript.pdf" />
  <link rel="alternate" type="application/pdf" href="https://greenelab.github.io/disparities_opinion/manuscript.pdf" />
  <link rel="alternate" type="text/html" href="https://greenelab.github.io/disparities_opinion/v/908c18f66f290c15e6558a7a3ed44f6236db033a/" />
  <meta name="manubot_html_url_versioned" content="https://greenelab.github.io/disparities_opinion/v/908c18f66f290c15e6558a7a3ed44f6236db033a/" />
  <meta name="manubot_pdf_url_versioned" content="https://greenelab.github.io/disparities_opinion/v/908c18f66f290c15e6558a7a3ed44f6236db033a/manuscript.pdf" />
  <meta property="og:type" content="article" />
  <meta property="twitter:card" content="summary_large_image" />
  <link rel="icon" type="image/png" sizes="192x192" href="https://manubot.org/favicon-192x192.png" />
  <link rel="mask-icon" href="https://manubot.org/safari-pinned-tab.svg" color="#ad1457" />
  <meta name="theme-color" content="#ad1457" />
  <!-- end Manubot generated metadata -->
bibliography:
- content/manual-references.json
manubot-output-bibliography: output/references.json
manubot-output-citekeys: output/citations.tsv
manubot-requests-cache-path: ci/cache/requests-cache
manubot-clear-requests-cache: false
...






<small><em>
This manuscript
([permalink](https://greenelab.github.io/disparities_opinion/v/908c18f66f290c15e6558a7a3ed44f6236db033a/))
was automatically generated
from [greenelab/disparities_opinion@908c18f](https://github.com/greenelab/disparities_opinion/tree/908c18f66f290c15e6558a7a3ed44f6236db033a)
on November 10, 2021.
</em></small>

## Authors



+ **John Doe**<br>
    ![ORCID icon](images/orcid.svg){.inline_icon width=16 height=16}
    [XXXX-XXXX-XXXX-XXXX](https://orcid.org/XXXX-XXXX-XXXX-XXXX)
    · ![GitHub icon](images/github.svg){.inline_icon width=16 height=16}
    [johndoe](https://github.com/johndoe)
    · ![Twitter icon](images/twitter.svg){.inline_icon width=16 height=16}
    [johndoe](https://twitter.com/johndoe)<br>
  <small>
     Department of Something, University of Whatever
     · Funded by Grant XXXXXXXX
  </small>

+ **Jane Roe**<br>
    ![ORCID icon](images/orcid.svg){.inline_icon width=16 height=16}
    [XXXX-XXXX-XXXX-XXXX](https://orcid.org/XXXX-XXXX-XXXX-XXXX)
    · ![GitHub icon](images/github.svg){.inline_icon width=16 height=16}
    [janeroe](https://github.com/janeroe)<br>
  <small>
     Department of Something, University of Whatever; Department of Whatever, University of Something
  </small>



Recognition can greatly affect the trajectory of a scientist’s career since each form of recognition paves the way to future accolades.
  Regardless of an individual’s merit, biases can skew which scientists are recognized.
  Audits can combat this by identifying overlooked groups and holding the recognizing body accountable.
  This is exemplified in a recent study of disparities in scientific journalism, which found that predicted East Asian names were under-quoted and under-mentioned [@doi:10.1101/2021.06.21.449261].

Auditing efforts can reduce disparities.
  Journalists found that self-audits of gender-based representation in their articles can help them approach parity [@https://www.theatlantic.com/technology/archive/2016/02/gender-diversity-journalism/463023/; @https://www.theatlantic.com/science/archive/2018/02/i-spent-two-years-trying-to-fix-the-gender-imbalance-in-my-stories/552404/].
  In award recognition, an audit of the International Society for Computational Biology (ISCB) honorees revealed significant under-representation of people with predicted East Asian name origins and over-representation of US-affiliated scientists.
  After this study was publicized, ISCB's following honorees had the highest mean predicted probability of having an East Asian name of any previous year, and the nominating committee inducted their first China-based ISCB Fellow [@doi:10.1016/j.cels.2021.07.007].

Ideally, audits would only use self-reported gender, ethnicity, and other identifications [@doi:10.1371/journal.pcbi.1003903].
  While this is possible prospectively, surveying is impractical for large groups and often impossible retrospectively [@http://mediashift.org/2014/11/how-to-ethically-and-responsibly-identify-gender-in-large-datasets/].
  In contrast, computationally-derived predictions allow for large-scale audits with as broad a scope as needed.
  Numerous tools exist to algorithmically infer gender, nationality, and ethnicity using the feature most likely to be present in datasets: an individual's name [@doi:10.1093/pan/mpw001; @http://genderize.io/].

Prediction models are not a panacea; several factors limit both their accuracy and utility.
  For instance, gender associations of a given name can vary by culture, potentially biasing gender predictions where additional information is not available [@https://aclanthology.org/U14-1021].
  Also, most gender prediction models are trained on binary gender labels, which systematically overlooks and conceptually erases the representation and contributions of transgender, non-binary, and intersex individuals [@doi:10.1101/2020.10.12.336230].

Proxy predictions of ethnicity via name origin are more difficult still; choosing categories to probabilistically predict on is non-trivial and difficult to discretize.
  Furthermore, there is no one-to-one mapping between having a name from a linguistic group and belonging to a minoritized or underrepresented group.
  Colonialism, immigration, and structural racism have affected linguistic histories and inclusion or exclusion from scientific communities in complex ways that are nearly impossible to parse from names alone.
  For instance, classifiers are usually unable to distinguish if names of Hispanic origin come from the Iberian Peninsula or from Latin America [@doi:10.1016/j.cels.2021.07.007].

Recognizing the aforementioned shortcomings, we propose recommendations for the creation and deployment of automated auditing tools:
  1. *Transparency.* Publicly provide all tools, code, and data used in the analysis.
  This enables public scrutiny to those being audited and those whose data are used.
  For confidential data, provide de-identified or aggregated data.

  2. *Individuals know best.* Self-identified demographic information should always be used in preference to algorithmic predictions.

  3. *Aggregates only.* Audit results should not affect individuals.
  Analyses must focus on aggregate estimations and any intermediary individual predictions should not be used externally.
  In addition, analysts must be mindful of hidden subpopulations that may be obscured during aggregation.

  4. *Inform the public.* While internal audits can help institutions reflect, researchers should inform the community of identified disparities.
  This should include a discussion of any caveats associated with the analyses performed (e.g., a binarization of gender) and the ethical implications of such choices.


## References {.page_break_before}

<!-- Explicitly insert bibliography here -->
<div id="refs"></div>
